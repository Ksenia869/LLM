import numpy as np
class HeadAttention(nn.Module):
    def __init__(self, emb_size: int, head_size: int, max_seq_len: int):
        super().__init__()
        self.emb_size = emb_size
        self.head_size = head_size
        self.max_seq_len = max_seq_len
        self.w_k = nn.Linear(emb_size, head_size)
        self.w_q = nn.Linear(emb_size, head_size)
        self.w_v = nn.Linear(emb_size, head_size)
        self.mask_attention = torch.tril(torch.ones(max_seq_len, max_seq_len))
        
    def forward(self, x):
        seq_len = x.shape[1]

        self.key_matrix = self.w_k(x)
        self.query_matrix = self.w_q(x)
        self.value_matrix = self.w_v(x)
        self.attention_matrix = torch.matmul(self.query_matrix, self.key_matrix.transpose(1, 2))
        self.attention_matrix /= np.sqrt(self.head_size)
        self.mask_matrix = self.mask_attention[:seq_len, :seq_len]
        self.attention_matrix = torch.where(self.mask_matrix == 1, self.attention_matrix, torch.tensor(float('-inf'), device=self.attention_matrix.device, dtype=self.attention_matrix.dtype))
       
        self.attention_matrix = torch.softmax(self.attention_matrix, dim=2)

        self.out = torch.matmul(self.attention_matrix, self.value_matrix)


        return self.out
    
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads: int, emb_size: int, head_size: int, max_seq_len: int, dropout: float = 0.1):
        super().__init__()
        self.num_heads = num_heads
        self.emb_size = emb_size
        self.head_size = head_size
        self.max_seq_len = max_seq_len

        self.heads = nn.ModuleList([HeadAttention(emb_size, head_size, max_seq_len) for i in range(num_heads)])
        self.linear = nn.Linear(head_size * num_heads, emb_size)
        self.drop = torch.nn.Dropout(p=dropout)

    def forward(self, x):
        head_output = []
        for head in self.heads:
            head_out = head(x)
            head_output.append(head_out)

        concat = torch.cat(head_output, dim=2)
        res_linear = self.linear(concat)
        output = self.drop(res_linear)

        return output

class FeedForward(nn.Module):
    def __init__(self, emb_size: int, dropout: float = 0.1):
        super().__init__()
        self.emb_size = emb_size

        self.l1 = nn.Linear(self.emb_size, 4 * self.emb_size)
        self.relu = nn.ReLU()
        self.l2 = nn.Linear(4 * self.emb_size, self.emb_size)
        self.drop = torch.nn.Dropout(p=dropout)

    def forward(self, x):

        x_l1 = self.l1(x)
        x_relu = self.relu(x_l1)
        x_l2 = self.l2(x_relu)
        x_drop = self.drop(x_l2)

        return x_drop
    
class Decoder(nn.Module):
    def __init__(self, num_heads:int, emb_size:int, head_size:int, max_seq_len: int, dropout: float=0.1):
        super().__init__()
        self.multi_head = MultiHeadAttention(num_heads=num_heads,
        emb_size=emb_size, head_size=head_size, max_seq_len=max_seq_len, dropout=dropout)

        self.feed_forward = FeedForward(emb_size=emb_size, dropout=dropout)

        self.ln1 = torch.nn.LayerNorm(emb_size)
        self.ln2 = torch.nn.LayerNorm(emb_size)
        
    def forward (self, x):
        x_multi = self.multi_head(x)
        x_sum1 = x_multi + x
        x_ln1 = self.ln1(x_sum1)
        x_ffn = self.feed_forward(x_ln1)
        x_sum2 = x_ffn + x_ln1
        x_ln2 = self.ln2(x_sum2)
        return x_ln2
    
class TokenEmbeddings(nn.Module):
    def __init__(self, vocab_size: int, emb_size: int):
        super().__init__()
        self.vocab_size = vocab_size
        self.emb_size = emb_size

        self.matrix_emb = nn.Embedding(vocab_size, emb_size)

    def forward(self, x: torch.Tensor):

        output = self.matrix_emb(x)
        
        return output
    
class PositionalEmbeddings(nn.Module):
    def __init__(self, max_seq_len: int, emb_size: int):
        super().__init__()
        self.max_seq_len = max_seq_len
        self.emb_size = emb_size

        self.matrix_emb = nn.Embedding(max_seq_len, emb_size)

    def forward(self, seq_len):

        output = self.matrix_emb(torch.arange(0, seq_len))
        
        return output

from torch.utils.data import Dataset
class GetData(Dataset):
    def __init__(self, data: list, seq_len: int, device: str='cpu'):
        self.data = data
        self.seq_len = seq_len
        self.device = device

    def __len__(self):
        return len(self.data) - self.seq_len - 1
    
    def __getitem__(self, idx: int):
        x = self.data[idx: idx + self.seq_len]
        y = self.data[idx + 1: idx + self.seq_len + 1]

        x_tensor = torch.tensor(x, dtype=torch.long, device=self.device)
        y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)

        return x_tensor, y_tensor
    
class GPT(nn.Module):
    def __init__(self, vocab_size: int, max_seq_len: int, emb_size: int, num_heads: int, head_size: int, num_layers: int, dropout: float=0.1, device: str='cpu'):
        super().__init__()

        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        self.emb_size = emb_size
        self.num_heads = num_heads
        self.head_size = head_size
        self.device = device
        self.num_layers = num_layers

        self.token_emb = TokenEmbeddings(vocab_size, emb_size)
        self.positional_emb = PositionalEmbeddings(max_seq_len, emb_size)
        self.drop = nn.Dropout(p=dropout)
        decoder_layers = []
        for _ in range(self.num_layers):
            decoder_layer = Decoder(num_heads, emb_size, head_size, max_seq_len, dropout)
            decoder_layers.append(decoder_layer)
        
        self.decoder_layers = nn.Sequential(*decoder_layers)
        self.ln = nn.Linear(emb_size, vocab_size)

    def forward(self, x):

        x_token = self.token_emb(x)
        x_positional = self.positional_emb(x.shape[1])
        x_res = x_token + x_positional
        x_drop = self.drop(x_res)
        x_decoder = self.decoder_layers(x_drop)
        x_ln = self.ln(x_decoder)

        return x_ln 
    
    def generate(self, x: torch.Tensor, max_new_tokens: int, do_sample: bool, temperature: float=1.0, top_k: int=None, top_p: float=None):
        for i in range(max_new_tokens):
            tokens = x[:, -self.max_seq_len:]
            logits = self.forward(tokens)/temperature

            if top_k is not None and do_sample == True:
                logits_last = logits[:, -1, :]
                top_k_values, top_k_indices = torch.sort(logits_last, dim=1, descending=True)
                min_value = top_k_values[:, top_k-1].unsqueeze(1)
                logits_last[logits_last < min_value] = -float('inf')
            if top_p is not None and do_sample == True:
                logits_last = logits[:, -1, :]
                probs = torch.softmax(logits_last, dim=1)
                sorted_probs, sorted_indices = torch.sort(probs, dim=1, descending=True)
                max_indices = sorted_indices[:, 0].unsqueeze(1)
                arg_indices = torch.argsort(sorted_indices, dim=1)
                cumsum = torch.cumsum(sorted_probs, dim=1)
                cumsum_probs = torch.gather(cumsum, dim=1, index=arg_indices)
                mask = cumsum_probs > top_p
                mask.scatter_(dim=1, index=max_indices, value=False)
                logits_last[mask] = -float('inf')

            probs = torch.softmax(logits[:, -1, :], dim=1)
            if do_sample == True:
                arg_log = torch.multinomial(probs, 1)
            else:
                arg_log  = torch.argmax(probs, dim=1)    
            arg_logits = torch.reshape(arg_log, (probs.shape[0], 1)) 
            x = torch.cat([x, arg_logits], dim=1)

        return x
    def fit(self, train_loader: DataLoader, valid_loader: DataLoader, num_epoch: int, learning_rate: float):

        self.to(self.device)
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)
        cross = nn.CrossEntropyLoss()
        train_loss = []
        val_loss = []

        #for i in tqdm(range(num_epoch)):
        for i in range(num_epoch):
            self.train()
            for inputs, targets in train_loader:
                results = self.forward(inputs)
                shapes = results.shape
                results = torch.reshape(results, (shapes[0]*shapes[1], shapes[2]))
                shapes = targets.shape
                targets = torch.reshape(targets, (shapes[0]*shapes[1],))
                self.loss= cross(results, targets)
                train_loss.append(self.loss.item())
                self.optimizer.zero_grad()
                self.loss.backward()
                self.optimizer.step()
            #train_losses = sum(train_loss)/len(train_loss)
            
            self.eval()
            with torch.no_grad():
                for inputs, targets in valid_loader:
                    results = self.forward(inputs)
                    shapes = results.shape
                    results = torch.reshape(results, (shapes[0]*shapes[1], shapes[2]))
                    shapes = targets.shape
                    targets = torch.reshape(targets, (shapes[0]*shapes[1],))
                    self.loss1= cross(results, targets)
                    val_loss.append(self.loss1.item())
                #val_losses = sum(val_loss)/len(val_loss)
            #print(f'Epoch {i+1}/{num_epoch}, Train Loss: {train_losses:.4f}, Val Loss: {val_losses:.4f}')
